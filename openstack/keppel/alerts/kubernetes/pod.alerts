# vim: set ft=yaml:

groups:
- name: openstack-keppel-pod.alerts
  rules:
      - alert: OpenstackKeppelPodCPUThrottling
        expr: rate(container_cpu_cfs_throttled_seconds_total{namespace="keppel"}[1h]) > 1
        for: 15m
        labels:
          no_alert_on_absence: "true" # small regions may have no throttled containers at all, so this may legitimately occur
          support_group: containers
          tier: os
          service: keppel
          severity: info
          context: cpu
          dashboard: keppel-overview
          meta: "{{ $labels.pod }}/{{ $labels.container }}"
        annotations:
          summary: Container is constantly CPU-throttled
          description: The container {{ $labels.pod }}/{{ $labels.container }} is being CPU-throttled
            constantly. This is probably impacting performance, so check if we can increase the number
            of replicas or the resource requests/limits.

      - alert: OpenstackKeppelPodSchedulingInsufficientMemory
        expr: sum(rate(kube_pod_failed_scheduling_memory_total{namespace="keppel"}[30m])) by (pod_name) > 0
        for: 15m
        labels:
          severity: warning
          support_group: containers
          tier: os
          service: keppel
          context: memory
          dashboard: keppel-overview
          meta: "{{ $labels.pod_name }}"
          no_alert_on_absence: "true" # the underlying metric is only generated when scheduling fails
        annotations:
          summary: Scheduling failed due to insufficient memory
          description: The pod {{ $labels.pod_name }} failed to be scheduled. Insufficient memory!

      - alert: OpenstackKeppelPodOOMKilled
        expr: sum(rate(klog_pod_oomkill{namespace="keppel",pod_name!~"keppel-janitor-.+"}[30m])) by (pod_name) > 0
        for: 5m
        labels:
          support_group: containers
          tier: os
          service: keppel
          severity: info
          context: memory
          dashboard: keppel-overview
          meta: "{{ $labels.pod_name }}"
          no_alert_on_absence: "true" # the underlying metric is only generated after the first oomkill
        annotations:
          summary: Pod was oomkilled
          description: The pod {{ $labels.pod_name }} was oomkilled recently

      - alert: OpenstackKeppelPodOOMExceedingLimits
        # NOTE: `container_memory_saturation_ratio` ranges from 0 to 1, so 0.7 = 70% and 1.0 = 100%
        # NOTE 2: keppel-janitor is excluded from this alert: we don't care if it OOMs every now and then as long as it's still doing its job (which is covered by application-specific alerts)
        expr: container_memory_saturation_ratio{namespace="keppel",pod_name!~"(?:keep-image-pulled|keppel-janitor)-.*"} > 0.7 AND predict_linear(container_memory_saturation_ratio{namespace="keppel",pod_name!~"(?:keep-image-pulled|keppel-janitor)-.*"}[1h], 7*3600) > 1.0
        for: 30m
        labels:
          support_group: containers
          tier: os
          service: keppel
          severity: info
          context: memory
          dashboard: keppel-overview
          meta: "{{ $labels.pod_name }}"
        annotations:
          summary: Exceeding memory limits in 8h
          description: The pod {{ $labels.pod_name }} will exceed its memory limit in 8h.
